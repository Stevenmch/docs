
# Pipeline PL_Gen_STG_DTL

## âœ… Resumen

Este pipeline estÃ¡ diseÃ±ado para ejecutar un trabajo de notebook de Spark para el staging de datos de forma genÃ©rica, sin depender de un framework de orquestaciÃ³n externo. Construye dinÃ¡micamente rutas de archivos, verifica la existencia de datos en una carpeta especificada y ejecuta un notebook de Spark si los datos estÃ¡n presentes. TambiÃ©n maneja escenarios de Ã©xito y fracaso, enviando eventos a un pipeline de callback.

## âš ï¸ Advertencias Importantes

*   **No modifiques la estructura de la variable `payload`:** El pipeline depende de una estructura especÃ­fica dentro de la variable `payload`, que se deriva del parÃ¡metro del pipeline `notebook_params`. Cambiar la estructura del JSON pasado en `notebook_params` probablemente causarÃ¡ errores en actividades como `getContainer`, `getCountry`, `getTableName` y `getTablePrefix`. Por ejemplo, si renombras `container_name` a `data_container`, la actividad `getContainer` fallarÃ¡ porque no encontrarÃ¡ la propiedad `container_name`.
*   **Evita cambiar la lÃ³gica de concatenaciÃ³n de la variable `file_path`:** La actividad `setFilePath` construye la ruta del archivo basÃ¡ndose en varias variables. Modificar la lÃ³gica de concatenaciÃ³n sin comprender sus implicaciones puede llevar a rutas de archivos incorrectas y problemas de acceso a datos. Por ejemplo, eliminar la cadena `'-datasets'` alterarÃ¡ la estructura de carpetas esperada.
*   **Ten cuidado al alterar la expresiÃ³n de la variable `current_date`:** La actividad `get current date` determina la fecha utilizada en la ruta del archivo. Cambiar la lÃ³gica, especialmente la funciÃ³n `addHours` o el formato de la fecha, puede llevar a que el pipeline busque datos en la particiÃ³n de fecha incorrecta.
*   **AsegÃºrate de que el dataset `DS_Binary_BrsPrjAdls` estÃ© configurado correctamente:** La actividad `checkFolderExists` utiliza este dataset para verificar la existencia de la carpeta de datos. Una configuraciÃ³n incorrecta de este dataset, como detalles de conexiÃ³n o configuraciÃ³n de formato de archivo incorrectos, harÃ¡ que el pipeline falle.

## ðŸž Posibles Errores Comunes

*   **Error:** El pipeline falla porque la carpeta no existe.
    *   **Causa:** La actividad `checkFolderExists` devuelve `false` porque la ruta de la carpeta construida en la actividad `setFilePath` es incorrecta, o los datos aÃºn no se han cargado en la ubicaciÃ³n esperada.
    *   **SoluciÃ³n:** Verifica los valores de las variables `container`, `country`, `table_prefix`, `table_name` y `current_date`. AsegÃºrate de que la estructura de carpetas en el data lake coincida con la ruta construida.
*   **Error:** El notebook de Spark no se ejecuta.
    *   **Causa:** La actividad `SparkNotebookPlayer` falla debido a problemas con el pool de Spark, la configuraciÃ³n del notebook o los parÃ¡metros pasados al notebook.
    *   **SoluciÃ³n:** Verifica el estado y la configuraciÃ³n del pool de Spark. Verifica que los parÃ¡metros `notebook_name`, `sparkpool_name` y `notebook_params` estÃ©n configurados correctamente. Revisa los logs del notebook para obtener mensajes de error especÃ­ficos.
*   **Error:** La variable `payload` no se analiza correctamente.
    *   **Causa:** La actividad `getPayloadObject` no puede analizar el parÃ¡metro `notebook_params` correctamente, a menudo debido a un formato JSON incorrecto o problemas de escape.
    *   **SoluciÃ³n:** AsegÃºrate de que el parÃ¡metro `notebook_params` sea una cadena JSON vÃ¡lida. Presta mucha atenciÃ³n al escape de caracteres especiales como las comillas. Utiliza un validador JSON para verificar el formato.
*   **Error:** Se estÃ¡ utilizando un formato de fecha incorrecto.
    *   **Causa:** La variable `current_date` no estÃ¡ en el formato esperado por la estructura de carpetas.
    *   **SoluciÃ³n:** AsegÃºrate de que la funciÃ³n `formatDateTime` en la actividad `get current date` estÃ© generando el formato correcto.

## ðŸ§© Detalles del Notebook NTB\_Gen\_Write\_CSV\_to\_Datalake

Este notebook estÃ¡ diseÃ±ado para ingerir genÃ©ricamente datos desde un data lake a una base de datos de Synapse Analytics. Soporta formatos de archivo CSV y JSON, aplicaciÃ³n de esquema, encriptaciÃ³n de datos y gestiÃ³n de particiones.

### Funcionalidad:

*   **AnÃ¡lisis de ParÃ¡metros:** Analiza los parÃ¡metros pasados desde el pipeline, incluyendo detalles de conexiÃ³n, informaciÃ³n de la tabla y configuraciÃ³n de archivos.
*   **Carga de Archivos:** Carga dinÃ¡micamente datos desde archivos CSV o JSON basÃ¡ndose en los parÃ¡metros proporcionados. Soporta la aplicaciÃ³n de esquemas, el manejo de esquemas faltantes y definiciones de esquemas personalizados.
*   **EncriptaciÃ³n de Datos:** Encripta columnas especificadas utilizando encriptaciÃ³n Fernet con claves almacenadas en Azure Key Vault.
*   **PreparaciÃ³n de Datos:** AÃ±ade columnas `processdate` y `businessdate` a los datos.
*   **GestiÃ³n de Particiones:** Gestiona particiones en la tabla de destino, eliminando las particiones existentes antes de cargar nuevos datos.
*   **OptimizaciÃ³n de Datos:** Optimiza el almacenamiento de datos reconstruyendo las particiones si es necesario.
*   **Manejo de Errores:** Proporciona mensajes de error detallados y actualizaciones de estado.

### ParÃ¡metros Clave:

El notebook depende del parÃ¡metro `notebook_params`, que es una cadena JSON que contiene las siguientes secciones:

*   **table\_params:**
    *   `database_name`: Nombre de la base de datos de destino.
    *   `table_name`: Nombre de la tabla de destino.
    *   `business_date`: Nombre de la columna para la fecha de negocio.
    *   `business_date_fmt`: Formato de la fecha de negocio. Puede ser `yyyy-MM-dd`, `timestamp` o un patrÃ³n regex si `business_date` es `filename`.
    *   `force_field_type`: Opcional. Permite forzar el tipo de un campo. Ejemplo: `[{'field1':'string'},{'field2':'int'}]`
    *   `force_field_name`: Opcional. Permite forzar el nombre de las columnas. Ejemplo: `['col1','col2']`
    *   `force_schema`: Opcional. Permite forzar un esquema especÃ­fico.
    *   `encrypt_columns`: Lista de columnas para encriptar.
    *   `flatten`: Opcional. Se utiliza para archivos JSON para especificar la columna a aplanar.
*   **params:**
    *   `table_name_prefix`: Prefijo para el nombre de la tabla.
    *   `container_name`: Nombre del contenedor en Azure Data Lake Storage.
    *   `file_type`: Tipo de archivo (CSV o JSON).
    *   `country`: CÃ³digo del paÃ­s.
    *   `file_path`: Ruta a los archivos de datos.
    *   `process_date`: Fecha del procesamiento de datos.
    *   `optimize`: Opcional. Si se establece, el notebook optimizarÃ¡ las particiones.
*   **read\_args:**
    *   Argumentos pasados al lector de Spark (por ejemplo, `sep`, `header`, `inferSchema`, `multiline`).

### Consideraciones Importantes:

*   **EncriptaciÃ³n de Datos:** AsegÃºrate de que Azure Key Vault estÃ© configurado correctamente y que el notebook tenga acceso a la clave Fernet.
*   **EvoluciÃ³n del Esquema:** El notebook asume un esquema consistente entre los archivos. Si el esquema cambia, el notebook puede fallar.
*   **Particionamiento:** El notebook utiliza `processdate` y `businessdate` para el particionamiento. AsegÃºrate de que estas columnas estÃ©n presentes y formateadas correctamente en los datos.
*   **Rutas de Archivos:** Verifica que las rutas de los archivos sean correctas y accesibles.
*   **Dependencias:** El notebook depende de la librerÃ­a `mssparkutils` y de `TokenLibrary` para acceder a los secretos de Azure Key Vault.

## ðŸ“Š Diagrama de Flujo (Mermaid.js)

```mermaid
graph TD;
    A[getPayloadObject] --> B(getContainer);
    A --> C(getCountry);
    A --> D(getTableName);
    A --> E(getTablePrefix);
    A --> F[get current date];
    B & C & D & E & F --> G(setFilePath);
    G --> H[checkFolderExists];
    H --> I{DataFolderExists?};
    I -- Yes --> J[setErrors];
    J --> K[SparkNotebookPlayer];
    K -- Succeeded --> L[AddMessageSuccess];
    K -- Failed --> M[AddMessageFail];
    L --> N[SendEventOK];
    M --> O[SendEventKO];
    N --> P((End));
    O --> Q[pipelineWithError];
    Q --> P;
    I -- No --> R[SendEventOK_noData];
    R --> P;
```
