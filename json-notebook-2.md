### 1.  Descripci贸n general del proyecto
- **Nombre del c贸digo:** NTB_Gen_Write_CSV_to_Datalake
- **Versi贸n:** No especificada
- **Explicaci贸n general:** Este c贸digo es un notebook de PySpark dise帽ado para leer archivos CSV o JSON desde un Data Lake Storage (ADLS), realizar transformaciones y escribir los datos en una tabla particionada en el Data Lake. Incluye funcionalidades para el manejo de esquemas, encriptaci贸n de columnas y optimizaci贸n de particiones.
- **Qu茅 problema resuelve el c贸digo:** Automatiza el proceso de ingesti贸n de datos desde archivos CSV o JSON a tablas particionadas en un Data Lake, manejando la variabilidad en los esquemas, la necesidad de encriptaci贸n y la optimizaci贸n del almacenamiento.

### 2. 锔 Visi贸n general del sistema
- **Arquitectura del sistema:**
```mermaid
graph LR
    A[Archivo CSV/JSON en ADLS] --> B(Notebook PySpark);
    B --> C{Validaci贸n y Transformaci贸n};
    C --> D[Tabla Parquet en ADLS];
    D --> E(Azure Synapse Analytics);
```
- **Tecnolog铆as utilizadas:**
  - PySpark
  - Azure Data Lake Storage (ADLS)
  - Azure Synapse Analytics
  - `mssparkutils`
  - `cryptography`
- **Dependencias:**
  - `pyspark`
  - `ast`
  - `json`
  - `datetime`
  - `math`
  - `re`
  - `uuid`
  - `cryptography`
- **Requisitos del sistema:**
  - Un entorno de Synapse Analytics Workspace configurado con un pool de Spark.
  - Acceso a una cuenta de Azure Data Lake Storage.
  - Permisos adecuados para leer y escribir en el Data Lake.
- **Prerrequisitos:**
  - Configuraci贸n de un pool de Spark en Azure Synapse Analytics.
  - Configuraci贸n de las credenciales de acceso a Azure Data Lake Storage.
  - Instalaci贸n de las bibliotecas de Python necesarias (`pyspark`, `cryptography`).
  - Existencia de un Key Vault para almacenar la clave de encriptaci贸n (si se utiliza la funcionalidad de encriptaci贸n).

### 3.  Gu铆a de uso
- **C贸mo usarlo:** El notebook se ejecuta en un entorno de Azure Synapse Analytics. Los par谩metros de configuraci贸n se pasan como un diccionario JSON a trav茅s de la variable `notebook_params`.
- **Explicaci贸n de los pasos (entrada, salida, par谩metros):**
  - **Entrada:** Archivos CSV o JSON ubicados en Azure Data Lake Storage.
  - **Salida:** Tablas Parquet particionadas en Azure Data Lake Storage.
  - **Par谩metros:**
    - `notebook_params` (JSON string): Contiene los siguientes par谩metros:
      - `read_args` (dict): Par谩metros para la lectura de los archivos (ej: `sep`, `header`, `inferSchema`, `multiline`).
      - `table_params` (dict): Par谩metros relacionados con la tabla de destino (ej: `database_name`, `table_name`, `business_date`, `business_date_fmt`, `encrypt_columns`, `flatten`).
      - `params` (dict): Par谩metros relacionados con el entorno (ej: `table_name_prefix`, `account_name`, `container_name`, `file_type`, `country`, `file_path`, `process_date`, `optimize`).
- **Caso de uso de ejemplo:**
```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import lit
import json

# Inicializar SparkSession
spark = SparkSession.builder.appName("EjemploEscrituraDatalake").getOrCreate()

# Definir los par谩metros
database_name = "mi_basededatos"
table_name = "mi_tabla"
container_name = "mi_contenedor"
account_name = "mi_cuenta_adls"
file_path = "/ruta/a/mi/archivo.csv"
process_date = "20240120"

# Crear un DataFrame de ejemplo
data = [("1", "Juan", "Perez"), ("2", "Maria", "Gomez")]
df = spark.createDataFrame(data, ["id", "nombre", "apellido"])

# Agregar columnas de auditor铆a
df = df.withColumn("processdate", lit(process_date))
df = df.withColumn("businessdate", lit(process_date))

# Ruta de destino en ADLS
output_path = f"abfss://{container_name}@{account_name}.dfs.core.windows.net/datalake/{database_name}/{table_name}"

# Escribir el DataFrame como tabla Parquet particionada
df.write.mode("overwrite").partitionBy("processdate", "businessdate").parquet(output_path)

print(f"Tabla {table_name} escrita exitosamente en {output_path}")

# Detener SparkSession
spark.stop()
```

### 5.  Referencias
- Documentaci贸n de PySpark: https://spark.apache.org/docs/latest/api/python/index.html
- Documentaci贸n de Azure Synapse Analytics: https://learn.microsoft.com/en-us/azure/synapse-analytics/
- Documentaci贸n de Azure Data Lake Storage: https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-introduction
