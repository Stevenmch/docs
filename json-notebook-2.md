```markdown
###  Descripci贸n general del proyecto
- **Nombre del c贸digo:** NTB_Gen_Write_CSV_to_Datalake
- **Versi贸n:** N/A
- **Explicaci贸n general:** Este notebook de Synapse Analytics en PySpark ingiere datos desde archivos CSV o JSON en un datalake, con opciones para transformaci贸n, encriptaci贸n y optimizaci贸n.
- **Qu茅 problema resuelve el c贸digo:** Automatiza la ingesta de datos desde diferentes fuentes de archivos hacia un datalake, manejando la creaci贸n de tablas, el particionamiento, la encriptaci贸n de datos sensibles y la optimizaci贸n del almacenamiento.

### 锔 Visi贸n general del sistema
- **Arquitectura del sistema:**
```mermaid
graph LR
A[Archivo CSV/JSON] --> B(Spark Dataframe)
B --> C{Transformaci贸n/Encriptaci贸n}
C --> D[Datalake (Parquet)]
D --> E((Tabla Externa/Interna))
```
- **Tecnolog铆as utilizadas:**
  - PySpark
  - Azure Synapse Analytics
  - Azure Datalake Storage (ADLS)
  - `mssparkutils`
  - `cryptography`
- **Dependencias:**
  - `pyspark`
  - `ast`
  - `json`
  - `datetime`
  - `math`
  - `re`
  - `uuid`
  - `cryptography`
- **Requisitos del sistema:**
  - Azure Synapse Analytics Workspace
  - Azure Datalake Storage Account
  - Spark Pool configurado en Synapse
- **Prerrequisitos:**
  - Permisos de acceso al Azure Datalake Storage.
  - Configuraci贸n de un Spark Pool en Azure Synapse Analytics.
  - Existencia de un Key Vault con la clave de encriptaci贸n (si se usa la encriptaci贸n).
  - Configuraci贸n de linked services para el Key Vault.

###  Gu铆a de uso
- **C贸mo usarlo:** El notebook se ejecuta en un entorno de Synapse Analytics Spark, configurando los par谩metros de ingesta a trav茅s de la variable `notebook_params`. Este proceso carga los datos desde el origen, los transforma seg煤n la configuraci贸n, y los escribe en el datalake en formato Parquet.
- **Explicaci贸n de los pasos (entrada, salida, par谩metros):**
  1.  **Configurar los par谩metros:** Define la variable `notebook_params` como un string en formato JSON que contiene la configuraci贸n de la ingesta. Por ejemplo: `notebook_params = "{'notebook_parameters':{'table_params':{'database_name': 'hub_datalake', 'table_name': 'vendors'},'params':{'account_name': 'azusst1voo929', 'container_name': 'temp', 'file_type': 'json', 'file_path': '/esp-datasets/brs-esp-navision/vendors', 'process_date': '20230412'},'read_args':{'sep': ',', 'header': True, 'inferSchema': True, 'multiline': True}}}"`.
  2.  **Ejecutar las celdas de c贸digo:** Ejecuta secuencialmente las celdas del notebook. La celda de par谩metros parsea el string JSON y configura las variables necesarias.
  3.  **Cargar los datos:** El c贸digo lee los archivos desde la ruta especificada en `file_path` del contenedor ADLS, utilizando las opciones definidas en `read_args`.
  4.  **Transformar y escribir los datos:** Los datos se transforman (ej: encriptaci贸n) y se escriben en el datalake en formato Parquet, particionados por `processdate` y `businessdate`. Se crea una tabla externa o interna en el metastore de Synapse.
  5.  **Verificar la ingesta:** Consulta la tabla creada en el datalake para verificar que los datos se hayan ingerido correctamente.
- **Caso de uso de ejemplo:**
```python
from pyspark.sql.functions import lit
# Asumiendo que 'data' es un DataFrame existente
process_date = "20240120"
data = data.withColumn("processdate", lit(process_date))

# Escribir el DataFrame en formato Parquet, particionado por processdate
output_path = "abfss://container@account.dfs.core.windows.net/path/to/output"
data.write.mode("overwrite").partitionBy("processdate").parquet(output_path)
```

###  Documentaci贸n de la API
- **Endpoints:** N/A
- **Formatos de solicitud y respuesta:** N/A
- **Autenticaci贸n y autorizaci贸n:** El acceso a Azure Datalake Storage se gestiona mediante la identidad del workspace de Synapse, que debe tener los roles adecuados (ej: Storage Blob Data Contributor) en la cuenta de almacenamiento.

###  Referencias
- [Azure Synapse Analytics documentation](https://learn.microsoft.com/en-us/azure/synapse-analytics/)
- [PySpark documentation](https://spark.apache.org/docs/latest/api/python/)
- [Azure Datalake Storage documentation](https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-introduction)
```
